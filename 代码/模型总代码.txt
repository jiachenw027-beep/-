import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, accuracy_score, f1_score,recall_score,recall_score
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler
import matplotlib.pyplot as plt
import seaborn as sns
import shutil
import os
from collections import defaultdict
from scipy.special import softmax
import joblib


import pandas as pd
import numpy as np
import torch
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import joblib
import uuid

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检查设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 加载所有数据集
all_datasets = [
    pd.read_csv('/personal/Diabetes Simple Diagnosis.csv'),  # 数据集1
    pd.read_csv('/personal/diabetes.csv'),                  # 数据集2
    pd.read_csv('/personal/diabetes_012_health_indicators_BRFSS2015.csv'),  # 数据集3
    pd.read_csv('/personal/diabetes_data.csv'),             # 数据集4
    pd.read_csv('/personal/diabetes_data_upload.csv'),      # 数据集5
    pd.read_csv('/personal/diabetes_dataset.csv'),          # 数据集6
    pd.read_csv('/personal/Diabetes_Dataset_With_18_Features.csv'),  # 数据集7
    pd.read_csv('/personal/diabetes_dataset00.csv'),        # 数据集8
    pd.read_csv('/personal/Diabetes_prediction.csv'),       # 数据集9
    pd.read_csv('/personal/diabetes_prediction_dataset.csv'),  # 数据集10
    pd.read_csv('/personal/Gestational Diabetes.csv'),      # 数据集11
    pd.read_csv('/personal/Gestational Diabetic Dat Set.csv'),  # 数据集12
    pd.read_csv('/personal/Healthcare-Diabetes.csv')        # 数据集13
]

# 多任务数据集（索引0、1、2、6、9、11、12）
multi_task_indices = [0, 1, 2, 6, 9, 11, 12]
datasets = [all_datasets[i] for i in multi_task_indices]
# 单任务数据集（索引3、4、5、8、10，任务ID 3、4、5、8、10）
single_task_indices = [3,5,8]
single_task_task_ids = [3,5,8]
single_task_datasets = [all_datasets[i] for i in single_task_indices]

# 任务8数据集（索引7）
dataset8 = all_datasets[7]
# 特征定义
initial_selected_features = [
    'Age', 'Gender', 'BMI', 'Pregnancy', 'Glucose', 'BloodPressure',
    'HbA1c_level', 'Smoking'
]
numeric_features = ['Age', 'BMI', 'Glucose', 'BloodPressure', 'HbA1c_level']
binary_features = ['Gender', 'Pregnancy', 'Smoking']

# 特征列名映射
feature_column_mapping = {
    'Age': ['Age', 'age'],
    'Gender': ['Gender', 'gender', 'Sex'],
    'BMI': ['BMI', 'bmi'],
    'Pregnancy': ['Pregnancy', 'Pregnancies', 'No of Pregnancy', 'Pregnancy No', 'GestationalDiabetes', 'Pregnancy History', 'No of Pregestation'],
    'Glucose': ['Glucose', 'glucose', 'FBS', 'blood_glucose_level', 'Blood_Glucose_Level', 'FPG', 'OGTT', 'FastingBloodSugar', 'Blood Glucose Levels'],
    'BloodPressure': ['BloodPressure', 'Blood Pressure', 'HighBP', 'systolic_bp', 'diastolic_bp', 'Sys BP', 'Dia BP', 'SBP', 'DBP',
                      'systolic_bp', 'diastolic_bp', 'High_BP'],
    'HbA1c_level': ['HbA1c_level', 'HbA1c', 'HDL', 'Insulin', 'Insulin_Level', 'Insulin Levels'],
    'Smoking': ['Smoking', 'smoking', 'Smoker', 'smoking_history', 'Smoking Status']
}

# 计算全局特征统计
def compute_feature_stats(datasets, numeric_features):
    feature_stats = {feature: {'means': [], 'stds': []} for feature in numeric_features}
    for idx, df in enumerate(datasets):
        df = df.copy()
        print(f"数据集 {idx + 1} 列名：{df.columns.tolist()}")  # 调试：打印列名
        # 特定数据集的预处理
        if idx == 0:  # Diabetes Simple Diagnosis.csv
            df = df[df['Gender'] != 'Other']
            df['Age'] = df['Age'].replace(0, np.nan)
        elif idx == 7:  # diabetes_dataset00.csv
            df['Age'] = df['Age'].replace(0, np.nan)
        elif idx == 8:  # Diabetes_prediction.csv
            df = df[df['Age'] >= 0]
            df['Glucose'] = df['Glucose'].replace(0, np.nan)
            df['BloodPressure'] = df['BloodPressure'].replace(0, np.nan)
        elif idx == 9:  # diabetes_prediction_dataset.csv
            df = df[df['gender'] != 'Other']
        elif idx == 11:  # Gestational Diabetic Dat Set.csv
            df['OGTT'] = df['OGTT'].replace(9999999, np.nan)
            df['Sys BP'] = df['Sys BP'].replace(9999999, np.nan)
            df['BMI'] = df['BMI'].replace(9999999, np.nan)
        elif idx == 12:  # Healthcare-Diabetes.csv
            df['Glucose'] = df['Glucose'].replace(0, np.nan)
            df['BloodPressure'] = df['BloodPressure'].replace(0, np.nan)
            df['BMI'] = df['BMI'].replace(0, np.nan)
        elif idx == 6:  # Diabetes_Dataset_With_18_Features.csv
            df['FPG'] = df['FPG'] * 18  # 转换为mg/dl

        feature_data = pd.DataFrame(index=df.index)
        for feature in numeric_features:
            col = None
            for col_name in feature_column_mapping[feature]:
                if col_name in df.columns:
                    col = df[col_name]
                    break
            if col is None:
                col = pd.Series(np.nan, index=df.index)
            if feature == 'Age' and idx == 2:  # diabetes_012_health_indicators_BRFSS2015.csv
                age_ranges = {
                    1: (18, 24), 2: (25, 29), 3: (30, 34), 4: (35, 39),
                    5: (40, 44), 6: (45, 49), 7: (50, 54), 8: (55, 59),
                    9: (60, 64), 10: (65, 69), 11: (70, 74), 12: (75, 79),
                    13: (80, 100)
                }
                col = col.apply(lambda x: np.random.randint(age_ranges[x][0], age_ranges[x][1] + 1) if x in age_ranges else np.nan)
            elif feature == 'BloodPressure':
                bp_col = pd.to_numeric(col, errors='coerce')
                if pd.Series(bp_col).isna().all():
                    sys_col = None
                    dia_col = None
                    for sys_name in ['systolic_bp', 'Sys BP', 'SBP', 'Sys BP']:
                        if sys_name in df.columns:
                            sys_col = pd.to_numeric(df[sys_name], errors='coerce')
                            break
                    for dia_name in ['diastolic_bp', 'Dia BP', 'DBP', 'Dia BP']:
                        if dia_name in df.columns:
                            dia_col = pd.to_numeric(df[dia_name], errors='coerce')
                            break
                    if sys_col is not None and dia_col is not None:
                        bp_col = (sys_col + dia_col) / 2
                col = bp_col
            feature_data[feature] = pd.to_numeric(col, errors='coerce', downcast='float').astype('float64')
        for feature in numeric_features:
            if feature in feature_data.columns and not feature_data[feature].isna().all():
                feature_stats[feature]['means'].append(feature_data[feature].mean())
                feature_stats[feature]['stds'].append(feature_data[feature].std())
    
    global_stats = {}
    for feature in numeric_features:
        if feature_stats[feature]['means']:
            global_stats[feature] = {
                'mean': np.mean(feature_stats[feature]['means']),
                'std': np.mean(feature_stats[feature]['stds'])
            }
        else:
            default_values = {
                'Age': {'mean': 30.0, 'std': 10.0},
                'BMI': {'mean': 25.0, 'std': 5.0},
                'Glucose': {'mean': 140.0, 'std': 30.0},
                'BloodPressure': {'mean': 120.0, 'std': 20.0},
                'HbA1c_level': {'mean': 5.0, 'std': 1.0}
            }
            global_stats[feature] = default_values[feature]
    return global_stats

global_feature_stats = compute_feature_stats(all_datasets, numeric_features)


# 血压特征工程
def generate_blood_pressure_features(df, label_col=None, global_feature_stats=None):
    """
    生成血压特征并返回MAP(平均动脉压)值来替代BloodPressure
    
    参数:
        df: 包含血压相关特征的DataFrame
        label_col: 用于判断高血压状态的标签列名(可选)
        global_feature_stats: 全局特征统计信息(可选，用于填补缺失值)
    
    返回:
        pd.Series: 计算得到的平均动脉压(MAP)值
    """
    # 定义可能的血压相关列名变体
    sbp_names = ['systolic_bp', 'SBP', 'Sys BP', 'BloodPressure', 'SystolicBP', 'Blood Pressure']
    dbp_names = ['diastolic_bp', 'DBP', 'Dia BP', 'DiastolicBP']
    hypertension_names = ['High_BP', 'HighBP', 'hypertension', 'Hypertension']
    
    # 初始化结果Series
    map_values = pd.Series(np.zeros(len(df)), index=df.index)
    
    # 情况1：数据集同时包含收缩压和舒张压
    sbp_col = next((col for col in sbp_names if col in df.columns and 'diastolic' not in col), None)
    dbp_col = next((col for col in dbp_names if col in df), None)
    
    if sbp_col and dbp_col:
        sbp = pd.to_numeric(df[sbp_col], errors='coerce').fillna(0)
        dbp = pd.to_numeric(df[dbp_col], errors='coerce').fillna(0)
        map_values = (sbp + 2 * dbp) / 3  # MAP计算公式
        return map_values
    
    # 情况2：数据集只有收缩压（包括 'BloodPressure' 视为收缩压）
    elif sbp_col:
        sbp = pd.to_numeric(df[sbp_col], errors='coerce').fillna(0)
        dbp = np.clip(sbp / 1.5, 60, 110)  # 估算舒张压
        map_values = (sbp + 2 * dbp) / 3
        return map_values
    
    # 情况3：数据集只有高血压标签
    hypertension_col = next((col for col in hypertension_names if col in df.columns), None)
    if hypertension_col is not None or label_col is not None:
        # 确定高血压状态
        if hypertension_col:
            hypertension = df[hypertension_col].astype(int)
        else:
            hypertension = df[label_col].map({
                'Positive': 1, 'Negative': 0, 'Diabetes': 1, 'No diabetes': 0,
                1: 1, 0: 0, 'Yes': 1, 'No': 0, 'Prediabetes': 1, 'GDM': 1, 
                'Non GDM': 0, 2: 1, 'True': 1, 'False': 0, 
                'positive': 1, 'negative': 0, 'YES': 1, 'NO': 0
            }).fillna(0).astype(int)
        
        # 生成收缩压
        default_mean = global_feature_stats['BloodPressure']['mean'] if global_feature_stats else 120
        default_std = global_feature_stats['BloodPressure']['std'] if global_feature_stats else 20
        sbp = np.where(hypertension == 0, 
                      np.clip(np.random.normal(default_mean - 10, default_std / 2, len(df)), 90, 120),
                      np.clip(np.random.normal(default_mean + 40, default_std, len(df)), 140, 180))
        
        # 估算舒张压并计算MAP
        dbp = np.clip(sbp / 1.5, 60, 110)
        map_values = (sbp + 2 * dbp) / 3
        return pd.Series(map_values, index=df.index)
    
    # 情况4：无明确血压信息，使用全局统计均值作为收缩压基值计算MAP
    default_mean = global_feature_stats['BloodPressure']['mean'] if global_feature_stats else 120
    default_std = global_feature_stats['BloodPressure']['std'] if global_feature_stats else 20
    sbp = np.clip(np.random.normal(default_mean, default_std / 2, len(df)), 90, 150)
    dbp = np.clip(sbp / 1.5, 60, 110)
    map_values = (sbp + 2 * dbp) / 3
    print(f"警告：数据集无明确血压信息，使用全局均值 {default_mean} 估算 MAP")
    return pd.Series(map_values, index=df.index)

# 数据预处理函数（部分更新）
def preprocess_dataset(df, selected_features, label_col, global_feature_stats, dataset_index, training=True):
    df = df.copy()
    print(f"数据集 {dataset_index + 1} 列名：{df.columns.tolist()}")  # 调试：打印列名
    
    # 特定数据集的预处理
    if dataset_index == 0:  # Diabetes Simple Diagnosis.csv
        df = df[df['Gender'] != 'Other']
        df['Age'] = df['Age'].replace(0, global_feature_stats['Age']['mean'])
    elif dataset_index == 7:  # diabetes_dataset00.csv
        df['Age'] = df['Age'].replace(0, global_feature_stats['Age']['mean'])
    elif dataset_index == 8:  # Diabetes_prediction.csv
        df = df[df['Age'] >= 0]
        df['Glucose'] = df['Glucose'].replace(0, global_feature_stats['Glucose']['mean'])
        df['BloodPressure'] = df['BloodPressure'].replace(0, global_feature_stats['BloodPressure']['mean'])
        df['Pregnancies'] = df['Pregnancies'].apply(lambda x: 1 if x > 0 else 0)
    elif dataset_index == 9:  # diabetes_prediction_dataset.csv
        df = df[df['gender'] != 'Other']
        if 'smoking_history' in df.columns:
            df['smoking_history'] = df['smoking_history'].map({'never': 0, 'No Info': 0}).fillna(1).astype(int)
    elif dataset_index == 11:  # Gestational Diabetic Dat Set.csv
        df['OGTT'] = df['OGTT'].replace(9999999, global_feature_stats['Glucose']['mean'])
        df['Sys BP'] = df['Sys BP'].replace(9999999, np.nan)
        df['BMI'] = df['BMI'].replace(9999999, global_feature_stats['BMI']['mean'])
        df['No of Pregnancy'] = df['No of Pregnancy'].apply(lambda x: 1 if x > 0 else 0)
    elif dataset_index == 12:  # Healthcare-Diabetes.csv
        df['Glucose'] = df['Glucose'].replace(0, global_feature_stats['Glucose']['mean'])
        df['BloodPressure'] = df['BloodPressure'].replace(0, global_feature_stats['BloodPressure']['mean'])
        df['BMI'] = df['BMI'].replace(0, global_feature_stats['BMI']['mean'])
        df['Pregnancies'] = df['Pregnancies'].apply(lambda x: 1 if x > 0 else 0)
    elif dataset_index == 6:  # Diabetes_Dataset_With_18_Features.csv
        df['FPG'] = df['FPG'] * 18
        if 'smoking' in df.columns:
            df['smoking'] = df['smoking'].apply(lambda x: 1 if x > 2 else 0)
        else:
            print(f"警告：数据集 {dataset_index + 1} 缺少 'smoking' 列，创建默认全 0 列")
            df['smoking'] = 0
        df['Gender'] = df['Gender'].map({1: 1, 2: 0})  # 1: Male, 2: Female
    elif dataset_index == 2:  # diabetes_012_health_indicators_BRFSS2015.csv
        age_ranges = {
            1: (18, 24), 2: (25, 29), 3: (30, 34), 4: (35, 39),
            5: (40, 44), 6: (45, 49), 7: (50, 54), 8: (55, 59),
            9: (60, 64), 10: (65, 69), 11: (70, 74), 12: (75, 79),
            13: (80, 100)
        }
        df['Age'] = df['Age'].apply(lambda x: np.random.randint(age_ranges[x][0], age_ranges[x][1] + 1) if x in age_ranges else np.nan)
        df['Sex'] = df['Sex'].map({0: 1, 1: 0})  # 0: Male, 1: Female

    feature_data = pd.DataFrame(index=df.index)
    
    for feature in numeric_features:
        col = None
        for col_name in feature_column_mapping[feature]:
            if col_name in df.columns:
                col = df[col_name]
                break
        if col is None:
            col = pd.Series(np.nan, index=df.index)
        if feature == 'BloodPressure':
            bp_col = pd.to_numeric(col, errors='coerce')
            if pd.Series(bp_col).isna().all():
                sys_col = None
                dia_col = None
                for sys_name in ['systolic_bp', 'Sys BP', 'SBP', 'SystolicBP']:
                    if sys_name in df.columns:
                        sys_col = pd.to_numeric(df[sys_name], errors='coerce')
                        break
                for dia_name in ['diastolic_bp', 'Dia BP', 'DBP', 'DiastolicBP']:
                    if dia_name in df.columns:
                        dia_col = pd.to_numeric(df[dia_name], errors='coerce')
                        break
                if sys_col is not None and dia_col is not None:
                    bp_col = (sys_col + 2 * dia_col) / 3
                else:
                    bp_col = generate_blood_pressure_features(df, label_col, global_feature_stats)
            col = bp_col
        feature_data[feature] = pd.to_numeric(col, errors='coerce').astype(np.float32)
    
    for feature in binary_features:
        col = None
        for col_name in feature_column_mapping[feature]:
            if col_name in df.columns:
                col = df[col_name]
                break
        if col is None:
            col = pd.Series(np.nan, index=df.index)
        if feature == 'Gender':
            feature_data['Gender'] = col.map({
                'Male': 1, 'Female': 0, 'male': 1, 'female': 0, 'Unknown': 0, 'M': 1, 'F': 0,
                1: 1, 0: 0, '1': 1, '0': 0
            }).fillna(0).astype(int)
        elif feature == 'Pregnancy':
            feature_data['Pregnancy'] = pd.to_numeric(col, errors='coerce').apply(
                lambda x: 1 if x > 0 else 0 if pd.notnull(x) else 0
            ).fillna(0).astype(int)
        elif feature == 'Smoking':
            feature_data['Smoking'] = col.map({
                0: 0, 1: 1, 'never': 0, 'current': 1, 'former': 0, 'not current': 0, 'ever': 1,
                'No': 0, 'Yes': 1, 'no': 0, 'yes': 1, 'Smoker': 1, 'Non-Smoker': 0, 'No Info': 0
            }).fillna(0).astype(int)
    
    for feature in selected_features:
        if feature not in feature_data.columns or feature_data[feature].isna().all():
            if feature in numeric_features:
                mean = global_feature_stats[feature]['mean']
                std = global_feature_stats[feature]['std']
                feature_data[feature] = np.random.normal(mean, std, size=len(feature_data)).astype(np.float32)
            else:
                feature_data[feature] = 0
    
    features = feature_data[selected_features]
    for feature in features.columns:
        if feature in numeric_features:
            features[feature] = pd.to_numeric(features[feature], errors='coerce').astype(np.float32)
        else:
            features[feature] = pd.to_numeric(features[feature], errors='coerce').astype(int)
    
    # 标签处理
    if label_col in df.columns:
        if label_col == 'Diabetes_012':
            df['Diagnosis'] = df[label_col].apply(lambda x: 1 if x in [1, 2] else 0 if x == 0 else np.nan)
        elif label_col == 'Class Label(GDM /Non GDM)':
            if df[label_col].dtype in ['int64', 'int32', 'float64', 'float32']:
                df['Diagnosis'] = df[label_col].astype(int)
            else:
                df['Diagnosis'] = df[label_col].str.lower().map({
                    'gdm': 1, 'non gdm': 0, 'gdm ': 1, 'non gdm ': 0
                })
        else:
            df['Diagnosis'] = df[label_col].map({
                'Positive': 1, 'Negative': 0, 'Diabetes': 1, 'No diabetes': 0,
                1: 1, 0: 0, 'Yes': 1, 'No': 0, 'Prediabetes': 1, 'GDM': 1, 'Non GDM': 0, 2: 1,
                'True': 1, 'False': 0, 'positive': 1, 'negative': 0, 'YES': 1, 'NO': 0
            })
        
        if df['Diagnosis'].isna().any():
            print(f"警告：数据集 {dataset_index + 1} 在 {label_col} 中有未映射的标签：{df[label_col][df['Diagnosis'].isna()].unique()}")
            df['Diagnosis'] = df['Diagnosis'].fillna(0).astype(int)
        
        labels = df['Diagnosis'].astype(int)
        if len(np.unique(labels)) <= 1:
            print(f"警告：数据集 {dataset_index + 1} 只有 {len(np.unique(labels))} 个类别。唯一标签：{labels.unique()}")
            return None, None
    else:
        print(f"错误：数据集 {dataset_index + 1} 未找到标签列 {label_col}。")
        return None, None
    
    return features, labels

# 多任务数据集处理
all_features = []
all_labels = []
scalers = []

for i, (dataset_idx, df) in enumerate(zip(multi_task_indices, datasets)):
    label_col = next((col for col in ['Diagnosis', 'diabetes', 'Outcome', 'Diabetes', 'Target', 
                        'Prediction', 'Class Label(GDM /Non GDM)', 'Diabetes_012', 'class']
                      if col in df.columns), None)
    
    if label_col:
        # 数据预处理
        features, labels = preprocess_dataset(df, initial_selected_features, label_col, global_feature_stats, dataset_idx)
        
        if features is not None and labels is not None:
            # 保存特征顺序
            feature_order = features.columns.tolist()
            
            X, y = features.values, labels.values
            
            # 平衡采样
            target_sample_size = int(np.median([len(d) for d in datasets]))
            if len(np.unique(y)) > 1:
                if len(X) < target_sample_size:
                    oversampler = RandomOverSampler(random_state=42)
                    X, y = oversampler.fit_resample(X, y)
                elif len(X) > target_sample_size:
                    undersampler = RandomUnderSampler(random_state=42)
                    X, y = undersampler.fit_resample(X, y)
            
            # 标准化数值特征
            scaler = StandardScaler()
            numeric_indices = [feature_order.index(f) for f in numeric_features if f in feature_order]
            X[:, numeric_indices] = scaler.fit_transform(X[:, numeric_indices])
            
            # 保存完整的scaler信息
            scaler_info = {
                'scaler': scaler,
                'feature_order': feature_order,
                'numeric_features': numeric_features,
                'dataset_index': i
            }
            
            scaler_path = f"/personal/new_scalers/multi_scaler_{i}.pkl"
            joblib.dump(scaler_info, scaler_path)
            print(f"保存多任务标准化器 {i}，特征顺序：{feature_order}，路径：{scaler_path}")
            
            # 追加到 all_features 和 all_labels
            all_features.append(X)
            all_labels.append(pd.Series(y))
        else:
            all_features.append(None)
            all_labels.append(None)

# 单任务数据集处理
single_task_features = [None] * len(single_task_indices)
single_task_labels = [None] * len(single_task_indices)
single_task_scalers = [None] * len(single_task_indices)

for i, (task_id, dataset_idx, df) in enumerate(zip(single_task_task_ids, single_task_indices, single_task_datasets)):
    label_col = next((col for col in ['Diagnosis', 'diabetes', 'Outcome', 'Diabetes', 'Target', 
                                      'Prediction', 'Class Label(GDM /Non GDM)', 'Diabetes_012', 'class']
                      if col in df.columns), None)
    
    if label_col:
        print(f"数据集 {dataset_idx + 1} - 标签列：{label_col}，唯一值：{df[label_col].unique()}")
        # 数据预处理
        features, labels = preprocess_dataset(df, initial_selected_features, label_col, global_feature_stats, dataset_idx)
        
        if features is not None and labels is not None:
            # 保存特征顺序
            feature_order = features.columns.tolist()
            
            X, y = features.values, labels.values
            
            # 平衡采样
            target_sample_size = int(np.median([len(d) for d in single_task_datasets]))
            if len(np.unique(y)) > 1:
                if len(X) < target_sample_size:
                    oversampler = RandomOverSampler(random_state=42)
                    X, y = oversampler.fit_resample(X, y)
                elif len(X) > target_sample_size:
                    undersampler = RandomUnderSampler(random_state=42)
                    X, y = undersampler.fit_resample(X, y)
            
            # 标准化数值特征
            scaler = StandardScaler()
            numeric_indices = [feature_order.index(f) for f in numeric_features if f in feature_order]
            X[:, numeric_indices] = scaler.fit_transform(X[:, numeric_indices])
            
            # 保存完整的scaler信息
            scaler_info = {
                'scaler': scaler,
                'feature_order': feature_order,
                'numeric_features': numeric_features,
                'task_id': task_id
            }
            
            scaler_path = f"/personal/new_scalers/single_task_scaler_{task_id}.pkl"
            joblib.dump(scaler_info, scaler_path)
            print(f"保存单任务标准化器 任务 {task_id}，特征顺序：{feature_order}，路径：{scaler_path}")
            
            # 存储处理结果
            single_task_features[i] = X
            single_task_labels[i] = pd.Series(y)
            single_task_scalers[i] = scaler_info
        else:
            print(f"警告：数据集 {dataset_idx + 1} 处理失败，跳过")
            single_task_features[i] = None
            single_task_labels[i] = None
            single_task_scalers[i] = None
    else:
        print(f"错误：数据集 {dataset_idx + 1} 未找到有效标签列")
        single_task_features[i] = None
        single_task_labels[i] = None
        single_task_scalers[i] = None

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检查设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 加载数据集 8
dataset8 = pd.read_csv('/personal/diabetes_dataset00.csv')

# 特征定义
type_features = [
    'Age', 'Blood Glucose Levels', 'Blood Pressure', 'Weight Gain During Pregnancy', 'BMI',
    'Digestive Enzyme Levels', 'Waist Circumference', 'Insulin Levels', 'Cholesterol Levels',
    'Pulmonary Function'
]

# 全局类型特征统计（与之前代码一致）
global_type_feature_stats = {
    'Age': {'mean': 30.0, 'std': 10.0},
    'Blood Glucose Levels': {'mean': 140.0, 'std': 30.0},
    'Blood Pressure': {'mean': 120.0, 'std': 20.0},
    'Weight Gain During Pregnancy': {'mean': 12.0, 'std': 5.0},
    'BMI': {'mean': 25.0, 'std': 5.0},
    'Digestive Enzyme Levels': {'mean': 100.0, 'std': 20.0},
    'Waist Circumference': {'mean': 90.0, 'std': 15.0},
    'Insulin Levels': {'mean': 10.0, 'std': 5.0},
    'Cholesterol Levels': {'mean': 200.0, 'std': 40.0},
    'Pulmonary Function': {'mean': 80.0, 'std': 10.0}
}

# 类型数据集预处理函数
def preprocess_type_dataset(df, selected_features, label_col, global_stats, training=True):
    df = df.copy()
    print(f"数据集 8 列名：{df.columns.tolist()}")  # 调试：打印列名
    
    feature_data = pd.DataFrame(index=df.index)
    
    # 处理数值特征
    for feature in selected_features:
        if feature in df.columns:
            col = pd.to_numeric(df[feature], errors='coerce').astype(np.float32)
        else:
            print(f"警告：数据集 8 缺少特征 '{feature}'，使用全局统计填充")
            mean = global_stats[feature]['mean']
            std = global_stats[feature]['std']
            col = np.random.normal(mean, std, size=len(df)).astype(np.float32)
        feature_data[feature] = col
    
    # 确保特征顺序
    features = feature_data[selected_features]
    
    # 标签处理
    if label_col in df.columns:
        # 打印 Target 列的唯一值以调试
        print(f"数据集 8 - 标签列：{label_col}，原始唯一值：{df[label_col].unique()}")
        
        # 糖尿病类型标签（13 种）
        type_labels = df[label_col].copy()
        
        # 定义 13 种糖尿病类型的映射（支持字符串和数值）
        type_mapping = {
            # 字符串映射
            'Cystic Fibrosis-Related Diabetes (CFRD)': 'CFRD',
            'Gestational Diabetes (GDM)': 'GDM',
            'LADA': 'LADA',
            'MODY': 'MODY',
            'Neonatal Diabetes Mellitus (NDM)': 'NDM',
            'Prediabetic': 'Prediabetic',
            'Secondary Diabetes': 'Secondary Diabetes',
            'Steroid-Induced Diabetes': 'Steroid-Induced Diabetes',
            'Type 1 Diabetes': 'Type 1 Diabetes',
            'Type 2 Diabetes': 'Type 2 Diabetes',
            'Type 3c Diabetes (Pancreatogenic Diabetes)': 'Type 3c Diabetes',
            'Wolfram Syndrome': 'Wolfram Syndrome',
            'Wolman Syndrome': 'Wolman Syndrome',
            # 数值映射（假设 0 到 12 对应 13 种类型）
            0: 'CFRD',
            1: 'GDM',
            2: 'LADA',
            3: 'MODY',
            4: 'NDM',
            5: 'Prediabetic',
            6: 'Secondary Diabetes',
            7: 'Steroid-Induced Diabetes',
            8: 'Type 1 Diabetes',
            9: 'Type 2 Diabetes',
            10: 'Type 3c Diabetes',
            11: 'Wolfram Syndrome',
            12: 'Wolman Syndrome'
        }
        
        # 将所有值转换为字符串（避免混合类型）
        type_labels = type_labels.astype(str)
        
        # 标准化标签名称
        type_labels = type_labels.map(type_mapping).fillna('Unknown')
        
        # 确保所有标签都是字符串
        unique_labels = sorted(type_labels.unique())
        type_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
        
        # 转换为数值标签
        diabetes_type_labels = type_labels.map(type_to_idx).astype(int)
        
        # 治疗标签（基于糖尿病类型分组）
        treatment_mapping = {
            # 无需特殊治疗
            'Prediabetic': 'No Special Treatment',
            # 胰岛素依赖
            'Type 1 Diabetes': 'Insulin-Dependent',
            'LADA': 'Insulin-Dependent',
            'CFRD': 'Insulin-Dependent',
            'NDM': 'Insulin-Dependent',
            'Wolfram Syndrome': 'Insulin-Dependent',
            'Wolman Syndrome': 'Insulin-Dependent',
            # 口服药物为主
            'Type 2 Diabetes': 'Oral Medications',
            'MODY': 'Oral Medications',
            'Secondary Diabetes': 'Oral Medications',
            'Steroid-Induced Diabetes': 'Oral Medications',
            'Type 3c Diabetes': 'Oral Medications',
            # 孕期管理
            'GDM': 'Gestational Management',
            # 未映射的标签
            'Unknown': 'Unknown Treatment'
        }
        
        treatment_labels = type_labels.map(treatment_mapping)
        unique_treatment_labels = sorted(treatment_labels.unique())
        treatment_to_idx = {label: idx for idx, label in enumerate(unique_treatment_labels)}
        
        # 转换为数值标签
        treatment_labels = treatment_labels.map(treatment_to_idx).astype(int)
        
        print(f"数据集 8 - 标准化后的类型标签唯一值：{type_labels.unique()}")
        print(f"糖尿病类型标签映射：{type_to_idx}")
        print(f"治疗标签映射：{treatment_to_idx}")
        
        if len(np.unique(diabetes_type_labels)) <= 1:
            print(f"警告：数据集 8 只有 {len(np.unique(diabetes_type_labels))} 个类别（糖尿病类型）。唯一标签：{unique_labels}")
            return None, None, None, None, None
        
        if len(np.unique(treatment_labels)) <= 1:
            print(f"警告：数据集 8 只有 {len(np.unique(treatment_labels))} 个类别（治疗类型）。唯一标签：{unique_treatment_labels}")
            return None, None, None, None, None
    else:
        print(f"错误：数据集 8 未找到标签列 {label_col}")
        return None, None, None, None, None
    
    return features, diabetes_type_labels, type_to_idx, treatment_labels, treatment_to_idx

# 预处理类型数据集
features, diabetes_type_labels, type_to_idx, treatment_labels, treatment_to_idx = preprocess_type_dataset(
    dataset8,
    type_features,
    'Target',
    global_type_feature_stats,
    training=True
)

if features is not None and diabetes_type_labels is not None and treatment_labels is not None:
    # 保存特征顺序
    feature_order = features.columns.tolist()
    print(f"类型数据集特征顺序：{feature_order}")
    
    # SMOTE 过采样（分别对 diabetes_type_labels 和 treatment_labels 进行）
    try:
        smote = SMOTE(random_state=42, k_neighbors=5)
        # 过采样 diabetes_type_labels
        features_resampled, type_labels_resampled = smote.fit_resample(features, diabetes_type_labels)
        
        # 重新构造 DataFrame 以对 treatment_labels 应用 SMOTE
        temp_df = features.copy()
        temp_df['type_label'] = diabetes_type_labels
        temp_df['treatment_label'] = treatment_labels
        
        # 过采样 treatment_labels
        smote = SMOTE(random_state=42, k_neighbors=5)
        features_resampled, treatment_labels_resampled = smote.fit_resample(
            temp_df.drop(columns=['type_label', 'treatment_label']),
            temp_df['treatment_label']
        )
        
        # 同步 type_labels_resampled（由于 SMOTE 可能生成不同样本，需重新对齐）
        # 这里假设 treatment_labels 的 SMOTE 结果可以近似对齐 type_labels
        # 更精确的做法需要联合多标签 SMOTE，但 imblearn 不直接支持
        # 因此，使用第一次 SMOTE 的 type_labels_resampled
    except ValueError as e:
        print(f"错误：SMOTE 过采样失败，可能由于类别样本不足。错误信息：{e}")
        features_resampled = features.values
        type_labels_resampled = diabetes_type_labels.values
        treatment_labels_resampled = treatment_labels.values
    
    # 标准化
    scaler = StandardScaler()
    features_resampled = pd.DataFrame(
        scaler.fit_transform(features_resampled),
        columns=feature_order
    )
    
    # 保存完整的 scaler 信息
    scaler_info = {
        'scaler': scaler,
        'feature_order': feature_order,
        'numeric_features': type_features,
        'model_type': 'diabetes_type'
    }
    
    scaler_path = "/personal/new_scalers/type_scaler.pkl"
    joblib.dump(scaler_info, scaler_path)
    print(f"保存类型数据集标准化器，特征顺序：{feature_order}，路径：{scaler_path}")
    
    # 转换为 Tensor
    features_tensor = torch.tensor(features_resampled.values, dtype=torch.float32).to(device)
    type_labels_tensor = torch.tensor(type_labels_resampled.values, dtype=torch.long).to(device)
    treatment_labels_tensor = torch.tensor(treatment_labels_resampled.values, dtype=torch.long).to(device)
else:
    print("错误：类型数据集预处理失败，跳过后续步骤")
    features_tensor = None
    type_labels_tensor = None
    treatment_labels_tensor = None

import torch
import torch.nn as nn
from torch.autograd import Function
class GradientReversal(Function):
    """
    梯度反转层（Gradient Reversal Layer）
    在前向传播时保持输入不变，反向传播时反转梯度方向
    """
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.alpha
        return output, None

# 将GRL注册为可调用的函数
gradient_reversal = GradientReversal.apply

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from collections import defaultdict
import numpy as np
import pandas as pd
import os
import shutil
from torch.optim.lr_scheduler import ReduceLROnPlateau

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检查设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 多任务模型定义
class DiabetesDiagnosisModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_tasks):
        super(DiabetesDiagnosisModel, self).__init__()
        self.shared_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.4)
        )
        self.task_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.BatchNorm1d(hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.4),
                nn.Linear(hidden_dim // 2, 2)
            ) for _ in range(num_tasks)
        ])
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x, task_id):
        shared_features = self.shared_layer(x)
        logits = self.task_heads[task_id](shared_features)
        return logits

# 数据清洗函数
def clean_data(features, labels):
    # 检查特征中的 nan 和 inf
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    # 确保标签是整数（0 或 1）
    labels = np.array(labels, dtype=np.int64)
    labels = np.clip(labels, 0, 1)  # 确保标签在 0 和 1 之间
    return features, labels

# 准备数据
multi_task_indices = [0, 1, 2, 6, 9, 11, 12]  # 多任务数据集索引
datasets = [None] * len(multi_task_indices)  # 占位符，实际应为数据集列表
train_loaders = []
val_loaders = []
class_weights_list = []

# 假设 all_features 和 all_labels 已从之前的预处理代码中获得
# all_features: 列表，包含 7 个任务的特征矩阵 (numpy 数组)
# all_labels: 列表，包含 7 个任务的标签 (pandas Series)

for task_id, (features, labels) in enumerate(zip(all_features, all_labels)):
    if features is None or labels is None:
        print(f"任务 {task_id} 数据无效，跳过")
        train_loaders.append(None)
        val_loaders.append(None)
        class_weights_list.append(None)
        continue
    
    # 清洗数据
    features, labels = clean_data(features, labels)
    
    # 打印标签分布
    label_counts = pd.Series(labels).value_counts()
    print(f"任务 {task_id} 标签分布: {label_counts.to_dict()}")
    
    # 计算类权重
    neg_count = (labels == 0).sum()
    pos_count = (labels == 1).sum()
    total = len(labels)
    if neg_count == 0 or pos_count == 0:
        print(f"警告：任务 {task_id} 类别不平衡，负类样本={neg_count}，正类样本={pos_count}，设置默认权重")
        weight_for_0 = 1.0
        weight_for_1 = 1.0
    else:
        weight_for_0 = (1 / neg_count) * total / 2.0
        weight_for_1 = (1 / pos_count) * total / 2.0
    class_weights = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float32).to(device)
    print(f"任务 {task_id} 类权重: 负类={weight_for_0:.4f}, 正类={weight_for_1:.4f}")
    class_weights_list.append(class_weights)
    
    # 转换为 Tensor
    features_tensor = torch.tensor(features, dtype=torch.float32)
    labels_tensor = torch.tensor(labels, dtype=torch.long)
    
    # 拆分训练集和验证集
    features_train, features_val, labels_train, labels_val = train_test_split(
        features_tensor, labels_tensor, test_size=0.2, random_state=42, stratify=labels_tensor
    )
    
    # 创建 DataLoader
    train_dataset = TensorDataset(features_train, labels_train)
    val_dataset = TensorDataset(features_val, labels_val)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    train_loaders.append(train_loader)
    val_loaders.append(val_loader)

# 定义损失函数
criteria = [
    nn.CrossEntropyLoss(weight=weights) if weights is not None else None
    for weights in class_weights_list
]

# 初始化模型和优化器
multi_model = DiabetesDiagnosisModel(
    input_dim=len(initial_selected_features),
    hidden_dim=128,
    num_tasks=len(multi_task_indices)
).to(device)

optimizer = optim.Adam(multi_model.parameters(), lr=0.0005, weight_decay=5e-4)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=2)
best_val_loss = float('inf')
patience = 5
counter = 0
best_model_path = "/personal/new_models/best_diabetes_multi_diagnosis_mode.pth"
final_model_path = "/personal/new_models/finall_diabetes_multi_diagnosis_mode.pth"

# 训练历史记录
history = {'train_loss': [], 'val_loss': [], 'metrics': defaultdict(list)}

# 训练循环
for epoch in range(50):
    multi_model.train()
    task_losses = [0.0] * len(multi_task_indices)
    task_counts = [0] * len(multi_task_indices)
    
    # 确保每个任务都被训练
    for task_id in range(len(multi_task_indices)):
        if train_loaders[task_id] is None:
            continue
        loader = train_loaders[task_id]
        for batch_features, batch_labels in loader:
            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)
            optimizer.zero_grad()
            logits = multi_model(batch_features, task_id)
            loss = criteria[task_id](logits, batch_labels)
            
            # 检查损失是否为 nan
            if torch.isnan(loss):
                print(f"警告：任务 {task_id} 在 Epoch {epoch+1} 产生 nan 损失，跳过该批次")
                continue
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(multi_model.parameters(), max_norm=1.0)
            optimizer.step()
            task_losses[task_id] += loss.item()
            task_counts[task_id] += 1
    
    # 计算平均训练损失
    avg_loss = sum(task_losses) / max(sum(task_counts), 1)
    if np.isnan(avg_loss):
        print(f"错误：Epoch {epoch+1} 平均训练损失为 nan，停止训练")
        break
    history['train_loss'].append(avg_loss)
    
    # 验证
    multi_model.eval()
    val_task_losses = [0.0] * len(multi_task_indices)
    val_task_counts = [0] * len(multi_task_indices)
    all_preds = [[] for _ in range(len(multi_task_indices))]
    all_targets = [[] for _ in range(len(multi_task_indices))]
    
    with torch.no_grad():
        for task_id in range(len(multi_task_indices)):
            if val_loaders[task_id] is None:
                continue
            for batch_features, batch_labels in val_loaders[task_id]:
                batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)
                logits = multi_model(batch_features, task_id)
                loss = criteria[task_id](logits, batch_labels)
                val_task_losses[task_id] += loss.item()
                val_task_counts[task_id] += 1
                preds = torch.argmax(logits, dim=1)
                all_preds[task_id].extend(preds.cpu().numpy())
                all_targets[task_id].extend(batch_labels.cpu().numpy())
    
    # 计算平均验证损失
    avg_val_loss = sum(val_task_losses) / max(sum(val_task_counts), 1)
    history['val_loss'].append(avg_val_loss)
    
    # 打印结果
    print(f"\nEpoch {epoch+1}/50 - Multi-Task Model")
    print(f"Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
    for task_id, dataset_idx in enumerate(multi_task_indices):
        if not all_targets[task_id]:
            continue
        precision = precision_score(all_targets[task_id], all_preds[task_id], average='weighted', zero_division=0)
        recall = recall_score(all_targets[task_id], all_preds[task_id], average='weighted', zero_division=0)
        f1 = f1_score(all_targets[task_id], all_preds[task_id], average='weighted', zero_division=0)
        task_val_loss = val_task_losses[task_id] / max(val_task_counts[task_id], 1)
        print(f"Dataset {dataset_idx + 1} (Task {task_id}): Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Loss: {task_val_loss:.4f}")
        history['metrics'][f'task{task_id}_precision'].append(precision)
        history['metrics'][f'task{task_id}_recall'].append(recall)
        history['metrics'][f'task{task_id}_f1'].append(f1)
    
    # 早停逻辑
    if np.isnan(avg_val_loss):
        print(f"错误：Epoch {epoch+1} 平均验证损失为 nan，停止训练")
        break
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        counter = 0
        torch.save(multi_model.state_dict(), best_model_path)
        print(f"New best model (Val Loss: {best_val_loss:.4f})")
    else:
        counter += 1
        if counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    scheduler.step(avg_val_loss)

# 保存最终模型
if os.path.exists(best_model_path):
    shutil.copy(best_model_path, final_model_path)
    os.remove(best_model_path)
    print(f"Multi-task model saved to: {final_model_path}")
else:
    print("警告：未找到最佳模型文件")


import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import joblib
import os
import shutil
from collections import defaultdict

# 设置随机种子
torch.manual_seed(42)
np.random.seed(42)

# 检查设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ================== 数据加载与初始化 ==================
def load_datasets():
    """加载所有数据集并返回列表"""
    dataset_paths = [
        '/personal/Diabetes Simple Diagnosis.csv',
        '/personal/diabetes.csv',
        '/personal/diabetes_012_health_indicators_BRFSS2015.csv',
        '/personal/diabetes_data.csv',
        '/personal/diabetes_data_upload.csv',
        '/personal/diabetes_dataset.csv',
        '/personal/Diabetes_Dataset_With_18_Features.csv',
        '/personal/diabetes_dataset00.csv',
        '/personal/Diabetes_prediction.csv',
        '/personal/diabetes_prediction_dataset.csv',
        '/personal/Gestational Diabetes.csv',
        '/personal/Gestational Diabetic Dat Set.csv',
        '/personal/Healthcare-Diabetes.csv'
    ]
    
    all_datasets = []
    for path in dataset_paths:
        try:
            df = pd.read_csv(path)
            # 统一处理列名：去除前后空格并转为小写
            df.columns = df.columns.str.strip().str.lower()
            all_datasets.append(df)
            print(f"成功加载数据集: {path}")
        except Exception as e:
            print(f"错误加载数据集 {path}: {str(e)}")
            all_datasets.append(None)
    return all_datasets

all_datasets = load_datasets()

# ================== 特征工程配置 ==================
initial_selected_features = [
    'age', 'gender', 'bmi', 'pregnancy', 'glucose', 
    'bloodpressure', 'hba1c_level', 'smoking'
]
numeric_features = ['age', 'bmi', 'glucose', 'bloodpressure', 'hba1c_level']
binary_features = ['gender', 'pregnancy', 'smoking']

# 增强版特征列名映射（全部转为小写）
feature_column_mapping = {
    'age': ['age'],
    'gender': ['gender', 'sex'],
    'bmi': ['bmi'],
    'pregnancy': ['pregnancy', 'pregnancies', 'no of pregnancy', 'pregnancy no', 
                 'gestationaldiabetes', 'pregnancy history', 'no of pregestation'],
    'glucose': ['glucose', 'fbs', 'blood_glucose_level', 'blood glucose levels', 
               'fpg', 'ogtt', 'fastingbloodsugar'],
    'bloodpressure': ['bloodpressure', 'blood pressure', 'highbp', 'high_bp', 
                     'systolic_bp', 'diastolic_bp', 'sys_bp', 'dia_bp', 
                     'sbp', 'dbp', 'sys bp', 'dia bp', 'hypertension'],
    'hba1c_level': ['hba1c_level', 'hba1c', 'hdl', 'insulin', 
                   'insulin_level', 'insulin levels'],
    'smoking': ['smoking', 'smoker', 'smoking_history', 'smoking status']
}

# ================== 血压特征工程 ==================
def generate_map_from_bp(df):
    """从现有血压数据计算MAP"""
    # 尝试获取收缩压和舒张压
    sbp_col = next((col for col in ['systolic_bp', 'sbp', 'sys_bp', 'sys bp'] if col in df.columns), None)
    dbp_col = next((col for col in ['diastolic_bp', 'dbp', 'dia_bp', 'dia bp'] if col in df.columns), None)
    
    if sbp_col and dbp_col:
        sbp = pd.to_numeric(df[sbp_col], errors='coerce')
        dbp = pd.to_numeric(df[dbp_col], errors='coerce')
        return (sbp + 2 * dbp) / 3
    elif sbp_col:
        sbp = pd.to_numeric(df[sbp_col], errors='coerce')
        dbp = np.clip(sbp / 1.5, 60, 110)
        return (sbp + 2 * dbp) / 3
    return None

def generate_blood_pressure_features(df, label_col=None):
    """统一生成血压特征(MAP)"""
    # 1. 尝试从现有血压数据计算
    map_values = generate_map_from_bp(df)
    if map_values is not None:
        return map_values.fillna(0)
    
    # 2. 从高血压标签生成
    hypertension_col = next((col for col in ['highbp', 'high_bp', 'hypertension'] if col in df.columns), None)
    if hypertension_col is not None:
        hypertension = df[hypertension_col].astype(int)
    elif label_col is not None:
        # 使用诊断标签作为高血压代理
        hypertension = df[label_col].map({
            'positive': 1, 'negative': 0, 'diabetes': 1, 'no diabetes': 0,
            1: 1, 0: 0, 'yes': 1, 'no': 0, 'prediabetes': 1, 'gdm': 1, 
            'non gdm': 0, 2: 1, 'true': 1, 'false': 0
        }).fillna(0).astype(int)
    else:
        # 3. 没有血压信息，返回默认正常值
        return pd.Series(np.full(len(df), 93), index=df.index)  # 93是正常MAP的近似值
    
    # 根据高血压状态生成收缩压
    sbp = np.where(hypertension == 0,
                  np.clip(np.random.normal(110, 5, len(df)), 90, 120),
                  np.clip(np.random.normal(160, 10, len(df)), 140, 180))
    
    # 估算舒张压并计算MAP
    dbp = np.clip(sbp / 1.5, 60, 110)
    return (sbp + 2 * dbp) / 3

# ================== 数据预处理 ==================
def preprocess_dataset(df, selected_features, label_col, global_stats, dataset_idx):
    """统一的数据预处理函数"""
    if df is None or df.empty:
        print(f"数据集 {dataset_idx + 1} 为空，跳过")
        return None, None
    
    df = df.copy()
    print(f"\n===== 预处理数据集 {dataset_idx + 1} =====")
    print("原始列名:", df.columns.tolist())
    
    # 特殊数据集处理
    if dataset_idx == 2:  # diabetes_012_health_indicators_BRFSS2015.csv
        # 处理年龄范围
        age_ranges = {1: (18,24), 2: (25,29), 3: (30,34), 4: (35,39),
                     5: (40,44), 6: (45,49), 7: (50,54), 8: (55,59),
                     9: (60,64), 10: (65,69), 11: (70,74), 12: (75,79), 13: (80,100)}
        df['age'] = df['age'].apply(lambda x: np.mean(age_ranges[x]) if x in age_ranges else np.nan)
    
    # 初始化特征DataFrame
    features = pd.DataFrame(index=df.index)
    
    # 处理每个特征
    for feature in selected_features:
        # 查找匹配的列
        col = None
        for col_name in feature_column_mapping.get(feature, [feature]):
            if col_name in df.columns:
                col = df[col_name]
                break
        
        # 特殊处理血压特征
        if feature == 'bloodpressure':
            features['bloodpressure'] = generate_blood_pressure_features(df, label_col)
            continue
            
        # 处理找到的列
        if col is not None:
            if feature in numeric_features:
                features[feature] = pd.to_numeric(col, errors='coerce')
            elif feature == 'gender':
                features[feature] = col.map({
                    'male':1, 'female':0, 'm':1, 'f':0, 1:1, 0:0
                }).fillna(0).astype(int)
            elif feature == 'pregnancy':
                features[feature] = pd.to_numeric(col, errors='coerce').apply(
                    lambda x: 1 if x > 0 else 0 if pd.notnull(x) else 0
                ).fillna(0).astype(int)
            elif feature == 'smoking':
                features[feature] = col.map({
                    0:0, 1:1, 'never':0, 'current':1, 'former':0, 'yes':1, 'no':0
                }).fillna(0).astype(int)
        else:
            # 缺失特征处理
            if feature in numeric_features:
                mean = global_stats[feature]['mean']
                std = global_stats[feature]['std']
                features[feature] = np.random.normal(mean, std, len(df))
            else:
                features[feature] = 0
    
    # 标签处理
    if label_col not in df.columns:
        print(f"错误：数据集 {dataset_idx + 1} 缺少标签列 {label_col}")
        return None, None
    
    labels = df[label_col].map({
        'Positive':1,'Negative':0,
        'positive':1, 'negative':0, 'diabetes':1, 'no diabetes':0,
        1:1, 0:0, 'yes':1, 'no':0, 'prediabetes':1, 'gdm':1, 'non gdm':0, 2:1,
        'true':1, 'false':0
    }).fillna(0).astype(int)
    
    if len(labels.unique()) <= 1:
        print(f"警告：数据集 {dataset_idx + 1} 只有1个类别")
        return None, None
    
    # 确保所有特征存在
    for feat in selected_features:
        if feat not in features.columns:
            features[feat] = 0
    
    return features[selected_features], labels

# ================== 模型定义 ==================
class SingleDiagnosisModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_tasks=1):
        super().__init__()
        self.shared_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.4)
        )
        self.task_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.BatchNorm1d(hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(0.4),
                nn.Linear(hidden_dim // 2, 2))
            for _ in range(num_tasks)
        ])
        
    def forward(self, x, task_id=0):
        shared = self.shared_layer(x)
        return self.task_heads[task_id](shared)

# ================== 训练流程 ==================
def train_single_task(task_id, dataset_idx, df):
    """训练单任务模型"""
    print(f"\n===== 训练任务 {task_id} (数据集 {dataset_idx + 1}) =====")
    
    # 获取标签列
    label_col = next((col for col in [
        'diagnosis', 'diabetes', 'outcome', 'target', 
        'prediction', 'class label(gdm /non gdm)', 'diabetes_012', 'class'
    ] if col in df.columns), None)
    
    if not label_col:
        print("错误：未找到有效标签列")
        return None
    
    # 数据预处理
    features, labels = preprocess_dataset(
        df, initial_selected_features, label_col, global_feature_stats, dataset_idx
    )
    
    if features is None or labels is None:
        return None
    
    # 检查类别数量
    unique_labels = np.unique(labels)
    print(f"标签分布: {dict(pd.Series(labels).value_counts())}")
    if len(unique_labels) <= 1:
        print(f"严重警告：数据集 {dataset_idx + 1} 只有 {len(unique_labels)} 个类别 ({unique_labels})")
        print("可能原因：")
        print("1. 原始数据本身只有一类（如所有样本都是糖尿病患者）")
        print("2. 标签映射错误导致所有标签被转换为同一类")
        print("3. 数据预处理过程中丢失了类别信息")
        return None
    
    # 平衡采样
    if len(unique_labels) > 1:
        sampler = RandomOverSampler() if len(features) < 1000 else RandomUnderSampler()
        X, y = sampler.fit_resample(features.values, labels.values)
    else:
        X, y = features.values, labels.values
    
    # 标准化
    scaler = StandardScaler()
    numeric_cols = [initial_selected_features.index(f) for f in numeric_features]
    X[:, numeric_cols] = scaler.fit_transform(X[:, numeric_cols])
    
    # 划分数据集
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # 创建DataLoader
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train), 
        torch.LongTensor(y_train))
    val_dataset = TensorDataset(
        torch.FloatTensor(X_val), 
        torch.LongTensor(y_val))
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32)
    
    # 初始化模型
    model = SingleDiagnosisModel(
        len(initial_selected_features), 128, num_tasks=1
    ).to(device)
    
    # 训练配置
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2)
    
    best_metrics = {'f1': 0, 'precision': 0, 'recall': 0}
    patience = 5
    no_improve = 0
    
    # 训练循环
    for epoch in range(50):
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch.to(device))
            loss = criterion(outputs, y_batch.to(device))
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # 验证
        model.eval()
        val_preds, val_true = [], []
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                outputs = model(X_batch.to(device))
                val_loss += criterion(outputs, y_batch.to(device)).item()
                val_preds.extend(torch.argmax(outputs, 1).cpu().numpy())
                val_true.extend(y_batch.numpy())
        
        # 计算所有指标
        val_precision = precision_score(val_true, val_preds, zero_division=0)
        val_recall = recall_score(val_true, val_preds, zero_division=0)
        val_f1 = f1_score(val_true, val_preds, zero_division=0)
        
        print(f"\nEpoch {epoch+1} 指标:")
        print(f"Train Loss: {train_loss/len(train_loader):.4f}")
        print(f"Val Loss:   {val_loss/len(val_loader):.4f}")
        print(f"Precision:  {val_precision:.4f}")
        print(f"Recall:     {val_recall:.4f}")
        print(f"F1 Score:   {val_f1:.4f}")
        
        # 早停与模型保存（基于F1分数）
        if val_f1 > best_metrics['f1']:
            best_metrics = {
                'f1': val_f1,
                'precision': val_precision,
                'recall': val_recall
            }
            no_improve = 0
            torch.save(model.state_dict(), f"/personal/new_models/best_single_task{task_id}.pth")
            print("✨ 发现新最佳模型，已保存")
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"⏹ 早停于epoch {epoch+1}，最佳F1分数: {best_metrics['f1']:.4f}")
                break
        
        scheduler.step(val_loss)
    
    # 最终输出最佳指标
    print("\n=== 训练完成 ===")
    print(f"最佳验证集指标 (任务 {task_id}):")
    print(f"Precision: {best_metrics['precision']:.4f}")
    print(f"Recall:    {best_metrics['recall']:.4f}")
    print(f"F1 Score:  {best_metrics['f1']:.4f}")
    
    return model

# ================== 主执行流程 ==================
if __name__ == "__main__":
    # 计算全局统计
    global_feature_stats = {
        'age': {'mean': 45, 'std': 15},
        'bmi': {'mean': 25, 'std': 5},
        'glucose': {'mean': 100, 'std': 20},
        'bloodpressure': {'mean': 120, 'std': 15},
        'hba1c_level': {'mean': 5.5, 'std': 1}
    }
    
    # 单任务训练
    single_task_indices = [3, 5, 8]  # 示例数据集索引
    for i, dataset_idx in enumerate(single_task_indices):
        df = all_datasets[dataset_idx]
        if df is not None:
            train_single_task(i, dataset_idx, df)


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import CosineAnnealingLR
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import pandas as pd
import numpy as np
import joblib
import os
import shutil

# 检查设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 加载 Dataset 8
df8 = pd.read_csv('/personal/diabetes_dataset00.csv')  # 替换为实际路径

# 特征重要性前10（均为数值特征）
selected_features = [
    'Age', 'Blood Glucose Levels', 'Blood Pressure', 'Weight Gain During Pregnancy', 'BMI',
    'Digestive Enzyme Levels', 'Waist Circumference', 'Insulin Levels', 'Cholesterol Levels',
    'Pulmonary Function'
]

# 数值特征（与 selected_features 相同）
numeric_features = selected_features

# 全局特征统计
global_feature_stats = {
    'Age': {'mean': 30.0, 'std': 10.0},
    'Blood Glucose Levels': {'mean': 140.0, 'std': 30.0},
    'Blood Pressure': {'mean': 120.0, 'std': 20.0},
    'Weight Gain During Pregnancy': {'mean': 12.0, 'std': 5.0},
    'BMI': {'mean': 25.0, 'std': 5.0},
    'Digestive Enzyme Levels': {'mean': 100.0, 'std': 20.0},
    'Waist Circumference': {'mean': 90.0, 'std': 15.0},
    'Insulin Levels': {'mean': 10.0, 'std': 5.0},
    'Cholesterol Levels': {'mean': 200.0, 'std': 40.0},
    'Pulmonary Function': {'mean': 80.0, 'std': 10.0}
}

# 数据预处理函数（训练阶段）
def preprocess_dataset(df, selected_features, label_col, global_feature_stats, training=True):
    df = df.copy()
    feature_data = pd.DataFrame(index=df.index)
    
    # 数值特征处理
    for feature in selected_features:
        col = df.get(feature, pd.Series(np.nan, index=df.index)).replace(9999999, np.nan)
        feature_data[feature] = pd.to_numeric(col, errors='coerce', downcast='float').astype('float64')
    
    # 缺失值填补
    for feature in selected_features:
        if feature not in feature_data.columns or feature_data[feature].isna().all():
            mean = global_feature_stats[feature]['mean']
            std = global_feature_stats[feature]['std']
            feature_data[feature] = np.random.normal(mean, std, size=len(feature_data))
        else:
            if training:
                feature_data[feature] = feature_data[feature].fillna(feature_data[feature].mean())
            else:
                mean = global_feature_stats[feature]['mean']
                feature_data[feature] = feature_data[feature].fillna(mean)
    
    features = feature_data[selected_features]
    
    if training:
        type_to_idx = {type_name: idx for idx, type_name in enumerate(df['Target'].unique())}
        diabetes_type_labels = df['Target'].map(type_to_idx).astype(int)
        
        # 治疗方案标签
        treatment_to_idx = {0: 'Diet + Exercise', 1: 'Metformin', 2: 'Insulin', 3: 'Metformin + Insulin'}
        treatment_labels = pd.Series(index=df.index, dtype=int)
        for idx in df.index:
            hba1c = df.loc[idx, 'Blood Glucose Levels'] * 0.05 if 'Blood Glucose Levels' in df.columns else 5.0
            diabetes_type = df.loc[idx, 'Target']
            if diabetes_type in ['Type 1 Diabetes', 'Neonatal Diabetes Mellitus (NDM)', 'MODY', 'Wolfram Syndrome', 'Type 3c Diabetes (Pancreatogenic Diabetes)']:
                treatment_labels[idx] = 2
            elif diabetes_type == 'Type 2 Diabetes':
                treatment_labels[idx] = 1 if hba1c <= 9.0 else 3
            elif diabetes_type == 'Gestational Diabetes':
                treatment_labels[idx] = 0 if hba1c < 7.0 else 2
            else:
                treatment_labels[idx] = 0 if hba1c < 7.0 else 3
        
        return features, diabetes_type_labels, type_to_idx, treatment_labels, treatment_to_idx
    else:
        return features

# 训练阶段预处理
features, diabetes_type_labels, type_to_idx, treatment_labels, treatment_to_idx = preprocess_dataset(df8, selected_features, 'Target', global_feature_stats, training=True)

# 重采样（SMOTE）
smote = SMOTE(random_state=42)
features_resampled, type_labels_resampled = smote.fit_resample(features, diabetes_type_labels)
treatment_labels_resampled = pd.Series([treatment_labels.iloc[np.where(diabetes_type_labels == t)[0][0]] for t in type_labels_resampled], index=type_labels_resampled.index)

# 标准化特征
scaler = StandardScaler()
features_resampled[numeric_features] = scaler.fit_transform(features_resampled[numeric_features])
features = features_resampled

# 保存 scaler
scaler_path = "/personal/pth/scaler_dataset8.pkl"
joblib.dump(scaler, scaler_path)
print(f"Saved scaler to: {scaler_path}")

# 转换为张量
features_tensor = torch.tensor(features.values, dtype=torch.float32).to(device)
type_labels_tensor = torch.tensor(type_labels_resampled.values, dtype=torch.long).to(device)
treatment_labels_tensor = torch.tensor(treatment_labels_resampled.values, dtype=torch.long).to(device)

# 构造伪域标签（基于 Blood Glucose Levels 分层）
blood_glucose_idx = selected_features.index('Blood Glucose Levels')
blood_glucose = features.iloc[:, blood_glucose_idx].values
domain_labels = np.zeros(len(blood_glucose), dtype=int)
domain_labels[(blood_glucose >= 140.0) & (blood_glucose <= 200.0)] = 1
domain_labels[blood_glucose > 200.0] = 2
domain_labels_tensor = torch.tensor(domain_labels, dtype=torch.long).to(device)

# 数据集划分
train_idx, val_idx = train_test_split(np.arange(len(features)), test_size=0.2, random_state=42, stratify=type_labels_resampled)
train_dataset = TensorDataset(features_tensor[train_idx], type_labels_tensor[train_idx], treatment_labels_tensor[train_idx], domain_labels_tensor[train_idx])
val_dataset = TensorDataset(features_tensor[val_idx], type_labels_tensor[val_idx], treatment_labels_tensor[val_idx], domain_labels_tensor[val_idx])
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

# 梯度反转层
class GradientReversal(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.alpha * grad_output, None

# 模型定义
class EnhancedDiabetesTypeModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_types, num_treatments, num_domains=3):
        super(EnhancedDiabetesTypeModel, self).__init__()
        self.shared_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim * 4),
            nn.BatchNorm1d(hidden_dim * 4),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim * 4, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.Softmax(dim=1)
        )
        self.type_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.BatchNorm1d(hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 4, num_types)
        )
        self.treatment_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_treatments)
        )
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, num_domains)
        )
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x, alpha=1.0):
        shared_features = self.shared_layer(x)
        attention_weights = self.attention(shared_features)
        weighted_features = shared_features * attention_weights
        type_logits = self.type_head(weighted_features)
        treatment_logits = self.treatment_head(weighted_features)
        domain_logits = self.domain_classifier(GradientReversal.apply(weighted_features, alpha))
        return type_logits, treatment_logits, domain_logits

# 初始化模型
input_dim = len(selected_features)
hidden_dim = 512
num_types = len(type_to_idx)
num_treatments = len(treatment_to_idx)
model = EnhancedDiabetesTypeModel(input_dim, hidden_dim, num_types, num_treatments, num_domains=3).to(device)

# 损失函数
type_weights = torch.zeros(num_types, dtype=torch.float32)
for idx in range(num_types):
    count = (type_labels_resampled == idx).sum()
    type_weights[idx] = len(type_labels_resampled) / (num_types * count) if count > 0 else 1.0
type_criterion = nn.CrossEntropyLoss(weight=type_weights.to(device))

treatment_weights = torch.zeros(num_treatments, dtype=torch.float32)
for idx in range(num_treatments):
    count = (treatment_labels_resampled == idx).sum()
    treatment_weights[idx] = len(treatment_labels_resampled) / (num_treatments * count) if count > 0 else 1.0
treatment_criterion = nn.CrossEntropyLoss(weight=treatment_weights.to(device))

domain_criterion = nn.CrossEntropyLoss()

# 优化器与调度器
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)

class WarmUpScheduler:
    def __init__(self, optimizer, warmup_epochs, base_lr):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.base_lr = base_lr
        self.current_epoch = 0
    
    def step(self):
        self.current_epoch += 1
        if self.current_epoch <= self.warmup_epochs:
            lr = self.base_lr * (self.current_epoch / self.warmup_epochs)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr

warmup_scheduler = WarmUpScheduler(optimizer, warmup_epochs=5, base_lr=0.001)
scheduler = CosineAnnealingLR(optimizer, T_max=20)

# 训练和验证
best_val_loss = float('inf')
patience = 5
counter = 0
best_model_path = "/personal/new_models/best_dataset8_diabetes_type_model.pth"
final_model_path = "/personal/new_models/dataset8_diabetes_type_model.pth"

history = {'train_loss': [], 'val_loss': [], 'type_acc': [], 'treat_acc': [], 'domain_acc': [], 'type_f1': [], 'treat_f1': []}
type_metrics_history = []
treat_consistency_history = []

for epoch in range(50):
    model.train()
    total_train_loss = 0.0
    train_type_preds = []
    train_type_targets = []
    train_treat_preds = []
    train_treat_targets = []
    train_domain_preds = []
    train_domain_targets = []
    
    for batch_features, batch_type_labels, batch_treat_labels, batch_domain_labels in train_loader:
        batch_features = batch_features.to(device)
        batch_type_labels = batch_type_labels.to(device)
        batch_treat_labels = batch_treat_labels.to(device)
        batch_domain_labels = batch_domain_labels.to(device)
        
        optimizer.zero_grad()
        
        # 前向传播
        alpha = 2.0 / (1.0 + np.exp(-10 * epoch / 50)) - 1  # 动态调整 alpha
        type_logits, treatment_logits, domain_logits = model(batch_features, alpha)
        
        # 计算损失
        loss_type = type_criterion(type_logits, batch_type_labels)
        loss_treatment = treatment_criterion(treatment_logits, batch_treat_labels)
        loss_domain = domain_criterion(domain_logits, batch_domain_labels)
        
        # 总损失
        loss = loss_type + 0.5 * loss_treatment + 0.1 * loss_domain
        
        # 反向传播
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_train_loss += loss.item()
        
        # 收集预测和真实标签
        train_type_preds.extend(torch.argmax(type_logits, dim=1).cpu().numpy())
        train_type_targets.extend(batch_type_labels.cpu().numpy())
        train_treat_preds.extend(torch.argmax(treatment_logits, dim=1).cpu().numpy())
        train_treat_targets.extend(batch_treat_labels.cpu().numpy())
        train_domain_preds.extend(torch.argmax(domain_logits, dim=1).cpu().numpy())
        train_domain_targets.extend(batch_domain_labels.cpu().numpy())
    
    avg_train_loss = total_train_loss / len(train_loader)
    train_type_acc = accuracy_score(train_type_targets, train_type_preds)
    train_treat_acc = accuracy_score(train_treat_targets, train_treat_preds)
    train_domain_acc = accuracy_score(train_domain_targets, train_domain_preds)
    
    # 验证阶段
    model.eval()
    total_val_loss = 0.0
    val_type_preds = []
    val_type_targets = []
    val_treat_preds = []
    val_treat_targets = []
    val_domain_preds = []
    val_domain_targets = []
    
    # 用于一致性评估：多次预测相同输入
    val_treat_preds_multiple = []  # 存储多次预测结果
    num_runs = 5  # 每次输入预测 5 次
    
    with torch.no_grad():
        for batch_features, batch_type_labels, batch_treat_labels, batch_domain_labels in val_loader:
            batch_features = batch_features.to(device)
            batch_type_labels = batch_type_labels.to(device)
            batch_treat_labels = batch_treat_labels.to(device)
            batch_domain_labels = batch_domain_labels.to(device)
            
            # 单次预测用于常规指标
            type_logits, treatment_logits, domain_logits = model(batch_features, alpha)
            
            loss_type = type_criterion(type_logits, batch_type_labels)
            loss_treatment = treatment_criterion(treatment_logits, batch_treat_labels)
            loss_domain = domain_criterion(domain_logits, batch_domain_labels)
            loss = loss_type + 0.5 * loss_treatment + 0.1 * loss_domain
            
            total_val_loss += loss.item()
            
            val_type_preds.extend(torch.argmax(type_logits, dim=1).cpu().numpy())
            val_type_targets.extend(batch_type_labels.cpu().numpy())
            val_treat_preds.extend(torch.argmax(treatment_logits, dim=1).cpu().numpy())
            val_treat_targets.extend(batch_treat_labels.cpu().numpy())
            val_domain_preds.extend(torch.argmax(domain_logits, dim=1).cpu().numpy())
            val_domain_targets.extend(batch_domain_labels.cpu().numpy())
            
            # 一致性评估：多次预测
            batch_treat_preds = []
            for _ in range(num_runs):
                _, treatment_logits, _ = model(batch_features, alpha)
                batch_treat_preds.append(torch.argmax(treatment_logits, dim=1).cpu().numpy())
            val_treat_preds_multiple.append(np.stack(batch_treat_preds, axis=0))
    
    avg_val_loss = total_val_loss / len(val_loader)
    val_type_acc = accuracy_score(val_type_targets, val_type_preds)
    val_treat_acc = accuracy_score(val_treat_targets, val_treat_preds)
    val_domain_acc = accuracy_score(val_domain_targets, val_domain_preds)
    val_type_f1 = f1_score(val_type_targets, val_type_preds, average='weighted')
    val_treat_f1 = f1_score(val_treat_targets, val_treat_preds, average='weighted')
    
    # 计算各糖尿病类型的精确率、召回率和 F1 分数
    precision, recall, f1, _ = precision_recall_fscore_support(val_type_targets, val_type_preds, average=None, labels=list(range(num_types)))
    type_metrics = {}
    idx_to_type = {idx: type_name for type_name, idx in type_to_idx.items()}
    for idx in range(num_types):
        type_name = idx_to_type[idx]
        type_metrics[type_name] = {
            'precision': precision[idx],
            'recall': recall[idx],
            'f1': f1[idx]
        }
    type_metrics_history.append(type_metrics)
    
    # 计算治疗建议一致性
    val_treat_preds_multiple = np.concatenate(val_treat_preds_multiple, axis=1)  # 形状 (num_runs, num_samples)
    consistency_scores = []
    for sample_idx in range(val_treat_preds_multiple.shape[1]):
        sample_preds = val_treat_preds_multiple[:, sample_idx]
        most_common_pred = np.bincount(sample_preds).argmax()
        consistency = np.mean(sample_preds == most_common_pred)
        consistency_scores.append(consistency)
    avg_consistency = np.mean(consistency_scores)
    treat_consistency_history.append(avg_consistency)
    
    # 记录历史
    history['train_loss'].append(avg_train_loss)
    history['val_loss'].append(avg_val_loss)
    history['type_acc'].append(val_type_acc)
    history['treat_acc'].append(val_treat_acc)
    history['domain_acc'].append(val_domain_acc)
    history['type_f1'].append(val_type_f1)
    history['treat_f1'].append(val_treat_f1)
    
    # 打印日志
    print(f"\nEpoch {epoch+1}/50")
    print(f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
    print(f"Type Acc: {val_type_acc:.4f}, Treat Acc: {val_treat_acc:.4f}, Domain Acc: {val_domain_acc:.4f}")
    print(f"Type F1: {val_type_f1:.4f}, Treat F1: {val_treat_f1:.4f}")
    print("\nPer Diabetes Type Metrics:")
    for type_name, metrics in type_metrics.items():
        print(f"{type_name}: Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1: {metrics['f1']:.4f}")
    print(f"\nTreatment Consistency: {avg_consistency:.4f}")
    
    # 学习率调整
    warmup_scheduler.step()
    if epoch >= 5:
        scheduler.step()
    
    # 早停机制
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        counter = 0
        torch.save(model.state_dict(), best_model_path)
        print(f"New best model (Val Loss: {best_val_loss:.4f})")
    else:
        counter += 1
        if counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

# 保存最终模型
shutil.copy(best_model_path, final_model_path)
if os.path.exists(best_model_path):
    os.remove(best_model_path)
print(f"Model saved to: {final_model_path}")

# 保存指标到 CSV
metrics_data = {
    'Epoch': list(range(1, len(history['train_loss']) + 1)),
    'Train Loss': history['train_loss'],
    'Val Loss': history['val_loss'],
    'Type Acc': history['type_acc'],
    'Treat Acc': history['treat_acc'],
    'Domain Acc': history['domain_acc'],
    'Type F1': history['type_f1'],
    'Treat F1': history['treat_f1'],
    'Treatment Consistency': treat_consistency_history
}
# 添加各类型的指标
for type_name in type_to_idx.keys():
    metrics_data[f'{type_name}_Precision'] = [type_metrics_history[i][type_name]['precision'] for i in range(len(type_metrics_history))]
    metrics_data[f'{type_name}_Recall'] = [type_metrics_history[i][type_name]['recall'] for i in range(len(type_metrics_history))]
    metrics_data[f'{type_name}_F1'] = [type_metrics_history[i][type_name]['f1'] for i in range(len(type_metrics_history))]

metrics_df = pd.DataFrame(metrics_data)
metrics_csv_path = "/personal/new_models/dataset8_metrics.csv"
metrics_df.to_csv(metrics_csv_path, index=False, float_format='%.4f')
print(f"Metrics saved to: {metrics_csv_path}")


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import CosineAnnealingLR
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import pandas as pd
import numpy as np
import joblib
import os
import shutil

# 检查设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 加载 Dataset 8
df8 = pd.read_csv('/personal/diabetes_dataset00.csv')  # 替换为实际路径

# 特征重要性前10（均为数值特征）
selected_features = [
    'Age', 'Blood Glucose Levels', 'Blood Pressure', 'Weight Gain During Pregnancy', 'BMI',
    'Digestive Enzyme Levels', 'Waist Circumference', 'Insulin Levels', 'Cholesterol Levels',
    'Pulmonary Function'
]

# 数值特征（与 selected_features 相同）
numeric_features = selected_features

# 全局特征统计
global_feature_stats = {
    'Age': {'mean': 30.0, 'std': 10.0},
    'Blood Glucose Levels': {'mean': 140.0, 'std': 30.0},
    'Blood Pressure': {'mean': 120.0, 'std': 20.0},
    'Weight Gain During Pregnancy': {'mean': 12.0, 'std': 5.0},
    'BMI': {'mean': 25.0, 'std': 5.0},
    'Digestive Enzyme Levels': {'mean': 100.0, 'std': 20.0},
    'Waist Circumference': {'mean': 90.0, 'std': 15.0},
    'Insulin Levels': {'mean': 10.0, 'std': 5.0},
    'Cholesterol Levels': {'mean': 200.0, 'std': 40.0},
    'Pulmonary Function': {'mean': 80.0, 'std': 10.0}
}

# 数据预处理函数（训练阶段）
def preprocess_dataset(df, selected_features, label_col, global_feature_stats, training=True):
    df = df.copy()
    feature_data = pd.DataFrame(index=df.index)
    
    # 数值特征处理
    for feature in selected_features:
        col = df.get(feature, pd.Series(np.nan, index=df.index)).replace(9999999, np.nan)
        feature_data[feature] = pd.to_numeric(col, errors='coerce', downcast='float').astype('float64')
    
    # 缺失值填补
    for feature in selected_features:
        if feature not in feature_data.columns or feature_data[feature].isna().all():
            mean = global_feature_stats[feature]['mean']
            std = global_feature_stats[feature]['std']
            feature_data[feature] = np.random.normal(mean, std, size=len(feature_data))
        else:
            if training:
                feature_data[feature] = feature_data[feature].fillna(feature_data[feature].mean())
            else:
                mean = global_feature_stats[feature]['mean']
                feature_data[feature] = feature_data[feature].fillna(mean)
    
    features = feature_data[selected_features]
    
    if training:
        type_to_idx = {type_name: idx for idx, type_name in enumerate(df['Target'].unique())}
        diabetes_type_labels = df['Target'].map(type_to_idx).astype(int)
        
        # 治疗方案标签
        treatment_to_idx = {0: 'Diet + Exercise', 1: 'Metformin', 2: 'Insulin', 3: 'Metformin + Insulin'}
        treatment_labels = pd.Series(index=df.index, dtype=int)
        for idx in df.index:
            hba1c = df.loc[idx, 'Blood Glucose Levels'] * 0.05 if 'Blood Glucose Levels' in df.columns else 5.0
            diabetes_type = df.loc[idx, 'Target']
            if diabetes_type in ['Type 1 Diabetes', 'Neonatal Diabetes Mellitus (NDM)', 'MODY', 'Wolfram Syndrome', 'Type 3c Diabetes (Pancreatogenic Diabetes)']:
                treatment_labels[idx] = 2
            elif diabetes_type == 'Type 2 Diabetes':
                treatment_labels[idx] = 1 if hba1c <= 9.0 else 3
            elif diabetes_type == 'Gestational Diabetes':
                treatment_labels[idx] = 0 if hba1c < 7.0 else 2
            else:
                treatment_labels[idx] = 0 if hba1c < 7.0 else 3
        
        return features, diabetes_type_labels, type_to_idx, treatment_labels, treatment_to_idx
    else:
        return features

# 训练阶段预处理
features, diabetes_type_labels, type_to_idx, treatment_labels, treatment_to_idx = preprocess_dataset(df8, selected_features, 'Target', global_feature_stats, training=True)

# 重采样（SMOTE）
smote = SMOTE(random_state=42)
features_resampled, type_labels_resampled = smote.fit_resample(features, diabetes_type_labels)
treatment_labels_resampled = pd.Series([treatment_labels.iloc[np.where(diabetes_type_labels == t)[0][0]] for t in type_labels_resampled], index=type_labels_resampled.index)

# 标准化特征
scaler = StandardScaler()
features_resampled[numeric_features] = scaler.fit_transform(features_resampled[numeric_features])
features = features_resampled

# 保存 scaler
scaler_path = "/personal/pth/scaler_dataset8.pkl"
joblib.dump(scaler, scaler_path)
print(f"Saved scaler to: {scaler_path}")

# 转换为张量
features_tensor = torch.tensor(features.values, dtype=torch.float32).to(device)
type_labels_tensor = torch.tensor(type_labels_resampled.values, dtype=torch.long).to(device)
treatment_labels_tensor = torch.tensor(treatment_labels_resampled.values, dtype=torch.long).to(device)

# 构造伪域标签（基于 Blood Glucose Levels 分层）
blood_glucose_idx = selected_features.index('Blood Glucose Levels')
blood_glucose = features.iloc[:, blood_glucose_idx].values
domain_labels = np.zeros(len(blood_glucose), dtype=int)
domain_labels[(blood_glucose >= 140.0) & (blood_glucose <= 200.0)] = 1
domain_labels[blood_glucose > 200.0] = 2
domain_labels_tensor = torch.tensor(domain_labels, dtype=torch.long).to(device)

# 数据集划分
train_idx, val_idx = train_test_split(np.arange(len(features)), test_size=0.2, random_state=42, stratify=type_labels_resampled)
train_dataset = TensorDataset(features_tensor[train_idx], type_labels_tensor[train_idx], treatment_labels_tensor[train_idx], domain_labels_tensor[train_idx])
val_dataset = TensorDataset(features_tensor[val_idx], type_labels_tensor[val_idx], treatment_labels_tensor[val_idx], domain_labels_tensor[val_idx])
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

# 模型定义（已提供）
class EnhancedDiabetesTypeModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_types, num_treatments, num_domains=3):
        super(EnhancedDiabetesTypeModel, self).__init__()
        self.shared_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim * 4),
            nn.BatchNorm1d(hidden_dim * 4),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim * 4, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.Softmax(dim=1)
        )
        self.type_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.BatchNorm1d(hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 4, num_types)
        )
        self.treatment_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_treatments)
        )
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, num_domains)
        )
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x, alpha=1.0):
        shared_features = self.shared_layer(x)
        attention_weights = self.attention(shared_features)
        weighted_features = shared_features * attention_weights
        type_logits = self.type_head(weighted_features)
        treatment_logits = self.treatment_head(weighted_features)
        domain_logits = self.domain_classifier(GradientReversal.apply(weighted_features, alpha))
        return type_logits, treatment_logits, domain_logits

# 初始化模型
input_dim = len(selected_features)
hidden_dim = 512
num_types = len(type_to_idx)
num_treatments = len(treatment_to_idx)
model = EnhancedDiabetesTypeModel(input_dim, hidden_dim, num_types, num_treatments, num_domains=3).to(device)

# 损失函数
type_weights = torch.zeros(num_types, dtype=torch.float32)
for idx in range(num_types):
    count = (type_labels_resampled == idx).sum()
    type_weights[idx] = len(type_labels_resampled) / (num_types * count) if count > 0 else 1.0
type_criterion = nn.CrossEntropyLoss(weight=type_weights.to(device))

treatment_weights = torch.zeros(num_treatments, dtype=torch.float32)
for idx in range(num_treatments):
    count = (treatment_labels_resampled == idx).sum()
    treatment_weights[idx] = len(treatment_labels_resampled) / (num_treatments * count) if count > 0 else 1.0
treatment_criterion = nn.CrossEntropyLoss(weight=treatment_weights.to(device))

domain_criterion = nn.CrossEntropyLoss()

# 优化器与调度器
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)

class WarmUpScheduler:
    def __init__(self, optimizer, warmup_epochs, base_lr):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.base_lr = base_lr
        self.current_epoch = 0
    
    def step(self):
        self.current_epoch += 1
        if self.current_epoch <= self.warmup_epochs:
            lr = self.base_lr * (self.current_epoch / self.warmup_epochs)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr

warmup_scheduler = WarmUpScheduler(optimizer, warmup_epochs=5, base_lr=0.001)
scheduler = CosineAnnealingLR(optimizer, T_max=20)

# 训练和验证
best_val_loss = float('inf')
patience = 5
counter = 0
best_model_path = "/personal/new_models/best_dataset8_diabetes_type_model.pth"
final_model_path = "/personal/new_models/dataset8_diabetes_type_model.pth"

history = {'train_loss': [], 'val_loss': [], 'type_acc': [], 'treat_acc': [], 'domain_acc': []}

for epoch in range(50):
    model.train()
    total_train_loss = 0.0
    train_type_preds = []
    train_type_targets = []
    train_treat_preds = []
    train_treat_targets = []
    train_domain_preds = []
    train_domain_targets = []
    
    for batch_features, batch_type_labels, batch_treat_labels, batch_domain_labels in train_loader:
        batch_features = batch_features.to(device)
        batch_type_labels = batch_type_labels.to(device)
        batch_treat_labels = batch_treat_labels.to(device)
        batch_domain_labels = batch_domain_labels.to(device)
        
        optimizer.zero_grad()
        
        # 前向传播
        alpha = 2.0 / (1.0 + np.exp(-10 * epoch / 50)) - 1  # 动态调整 alpha
        type_logits, treatment_logits, domain_logits = model(batch_features, alpha)
        
        # 计算损失
        loss_type = type_criterion(type_logits, batch_type_labels)
        loss_treatment = treatment_criterion(treatment_logits, batch_treat_labels)
        loss_domain = domain_criterion(domain_logits, batch_domain_labels)
        
        # 总损失
        loss = loss_type + 0.5 * loss_treatment + 0.1 * loss_domain
        
        # 反向传播
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_train_loss += loss.item()
        
        # 收集预测和真实标签
        train_type_preds.extend(torch.argmax(type_logits, dim=1).cpu().numpy())
        train_type_targets.extend(batch_type_labels.cpu().numpy())
        train_treat_preds.extend(torch.argmax(treatment_logits, dim=1).cpu().numpy())
        train_treat_targets.extend(batch_treat_labels.cpu().numpy())
        train_domain_preds.extend(torch.argmax(domain_logits, dim=1).cpu().numpy())
        train_domain_targets.extend(batch_domain_labels.cpu().numpy())
    
    avg_train_loss = total_train_loss / len(train_loader)
    train_type_acc = accuracy_score(train_type_targets, train_type_preds)
    train_treat_acc = accuracy_score(train_treat_targets, train_treat_preds)
    train_domain_acc = accuracy_score(train_domain_targets, train_domain_preds)
    
    # 验证阶段
    model.eval()
    total_val_loss = 0.0
    val_type_preds = []
    val_type_targets = []
    val_treat_preds = []
    val_treat_targets = []
    val_domain_preds = []
    val_domain_targets = []
    
    with torch.no_grad():
        for batch_features, batch_type_labels, batch_treat_labels, batch_domain_labels in val_loader:
            batch_features = batch_features.to(device)
            batch_type_labels = batch_type_labels.to(device)
            batch_treat_labels = batch_treat_labels.to(device)
            batch_domain_labels = batch_domain_labels.to(device)
            
            type_logits, treatment_logits, domain_logits = model(batch_features, alpha)
            
            loss_type = type_criterion(type_logits, batch_type_labels)
            loss_treatment = treatment_criterion(treatment_logits, batch_treat_labels)
            loss_domain = domain_criterion(domain_logits, batch_domain_labels)
            loss = loss_type + 0.5 * loss_treatment + 0.1 * loss_domain
            
            total_val_loss += loss.item()
            
            val_type_preds.extend(torch.argmax(type_logits, dim=1).cpu().numpy())
            val_type_targets.extend(batch_type_labels.cpu().numpy())
            val_treat_preds.extend(torch.argmax(treatment_logits, dim=1).cpu().numpy())
            val_treat_targets.extend(batch_treat_labels.cpu().numpy())
            val_domain_preds.extend(torch.argmax(domain_logits, dim=1).cpu().numpy())
            val_domain_targets.extend(batch_domain_labels.cpu().numpy())
    
    avg_val_loss = total_val_loss / len(val_loader)
    val_type_acc = accuracy_score(val_type_targets, val_type_preds)
    val_treat_acc = accuracy_score(val_treat_targets, val_treat_preds)
    val_domain_acc = accuracy_score(val_domain_targets, val_domain_preds)
    val_type_f1 = f1_score(val_type_targets, val_type_preds, average='weighted')
    val_treat_f1 = f1_score(val_treat_targets, val_treat_preds, average='weighted')
    
    # 记录历史
    history['train_loss'].append(avg_train_loss)
    history['val_loss'].append(avg_val_loss)
    history['type_acc'].append(val_type_acc)
    history['treat_acc'].append(val_treat_acc)
    history['domain_acc'].append(val_domain_acc)
    
    # 打印日志
    print(f"\nEpoch {epoch+1}/50")
    print(f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
    print(f"Type Acc: {val_type_acc:.4f}, Treat Acc: {val_treat_acc:.4f}, Domain Acc: {val_domain_acc:.4f}")
    print(f"Type F1: {val_type_f1:.4f}, Treat F1: {val_treat_f1:.4f}")
    
    # 学习率调整
    warmup_scheduler.step()
    if epoch >= 5:
        scheduler.step()
    
    # 早停机制
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        counter = 0
        torch.save(model.state_dict(), best_model_path)
        print(f"New best model (Val Loss: {best_val_loss:.4f})")
    else:
        counter += 1
        if counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

# 保存最终模型
shutil.copy(best_model_path, final_model_path)
if os.path.exists(best_model_path):
    os.remove(best_model_path)
print(f"Model saved to: {final_model_path}")


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import CosineAnnealingLR
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import pandas as pd
import numpy as np
import joblib
import os
import shutil

# 自定义学习率预热调度器
class WarmUpScheduler:
    def __init__(self, optimizer, warmup_epochs, base_lr):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.base_lr = base_lr
        self.current_epoch = 0
    
    def step(self):
        self.current_epoch += 1
        if self.current_epoch <= self.warmup_epochs:
            lr = self.base_lr * (self.current_epoch / self.warmup_epochs)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr

# 检查设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 加载 Dataset 8
df8 = pd.read_csv('/personal/diabetes_dataset00.csv')

# 特征重要性前10
selected_features = [
    'Age', 'Blood Glucose Levels', 'Blood Pressure', 'Weight Gain During Pregnancy', 'BMI',
    'Digestive Enzyme Levels', 'Waist Circumference', 'Insulin Levels', 'Cholesterol Levels',
    'Pulmonary Function'
]

numeric_features = selected_features

global_feature_stats = {
    'Age': {'mean': 30.0, 'std': 10.0},
    'Blood Glucose Levels': {'mean': 140.0, 'std': 30.0},
    'Blood Pressure': {'mean': 120.0, 'std': 20.0},
    'Weight Gain During Pregnancy': {'mean': 12.0, 'std': 5.0},
    'BMI': {'mean': 25.0, 'std': 5.0},
    'Digestive Enzyme Levels': {'mean': 100.0, 'std': 20.0},
    'Waist Circumference': {'mean': 90.0, 'std': 15.0},
    'Insulin Levels': {'mean': 10.0, 'std': 5.0},
    'Cholesterol Levels': {'mean': 200.0, 'std': 40.0},
    'Pulmonary Function': {'mean': 80.0, 'std': 10.0}
}

def preprocess_dataset(df, selected_features, label_col, global_feature_stats, training=True):
    df = df.copy()
    feature_data = pd.DataFrame(index=df.index)
    
    for feature in selected_features:
        col = df.get(feature, pd.Series(np.nan, index=df.index)).replace(9999999, np.nan)
        feature_data[feature] = pd.to_numeric(col, errors='coerce', downcast='float').astype('float64')
    
    for feature in selected_features:
        if feature not in feature_data.columns or feature_data[feature].isna().all():
            mean = global_feature_stats[feature]['mean']
            std = global_feature_stats[feature]['std']
            feature_data[feature] = np.random.normal(mean, std, size=len(feature_data))
        else:
            if training:
                feature_data[feature] = feature_data[feature].fillna(feature_data[feature].mean())
            else:
                mean = global_feature_stats[feature]['mean']
                feature_data[feature] = feature_data[feature].fillna(mean)
    
    features = feature_data[selected_features]
    
    if training:
        type_to_idx = {type_name: idx for idx, type_name in enumerate(df['Target'].unique())}
        diabetes_type_labels = df['Target'].map(type_to_idx).astype(int)
        
        treatment_to_idx = {0: 'Diet + Exercise', 1: 'Metformin', 2: 'Insulin', 3: 'Metformin + Insulin'}
        treatment_labels = pd.Series(index=df.index, dtype=int)
        for idx in df.index:
            hba1c = df.loc[idx, 'Blood Glucose Levels'] * 0.05 if 'Blood Glucose Levels' in df.columns else 5.0
            diabetes_type = df.loc[idx, 'Target']
            if diabetes_type in ['Type 1 Diabetes', 'Neonatal Diabetes Mellitus (NDM)', 'MODY', 'Wolfram Syndrome', 'Type 3c Diabetes (Pancreatogenic Diabetes)']:
                treatment_labels[idx] = 2
            elif diabetes_type == 'Type 2 Diabetes':
                treatment_labels[idx] = 1 if hba1c <= 9.0 else 3
            elif diabetes_type == 'Gestational Diabetes':
                treatment_labels[idx] = 0 if hba1c < 7.0 else 2
            else:
                treatment_labels[idx] = 0 if hba1c < 7.0 else 3
        
        return features, diabetes_type_labels, type_to_idx, treatment_labels, treatment_to_idx
    else:
        return features

# 预处理
features, diabetes_type_labels, type_to_idx, treatment_labels, treatment_to_idx = preprocess_dataset(df8, selected_features, 'Target', global_feature_stats, training=True)

# SMOTE 重采样
smote = SMOTE(random_state=42)
features_resampled, type_labels_resampled = smote.fit_resample(features, diabetes_type_labels)
treatment_labels_resampled = pd.Series([treatment_labels.iloc[np.where(diabetes_type_labels == t)[0][0]] for t in type_labels_resampled], index=type_labels_resampled.index)

# 标准化
scaler = StandardScaler()
features_resampled[numeric_features] = scaler.fit_transform(features_resampled[numeric_features])
features = features_resampled

scaler_path = "/personal/pth/scaler_dataset8.pkl"
joblib.dump(scaler, scaler_path)
print(f"Saved scaler to: {scaler_path}")

# 转换为张量
features_tensor = torch.tensor(features.values, dtype=torch.float32).to(device)
type_labels_tensor = torch.tensor(type_labels_resampled.values, dtype=torch.long).to(device)
treatment_labels_tensor = torch.tensor(treatment_labels_resampled.values, dtype=torch.long).to(device)

# 伪域标签
blood_glucose_idx = selected_features.index('Blood Glucose Levels')
blood_glucose = features.iloc[:, blood_glucose_idx].values
domain_labels = np.zeros(len(blood_glucose), dtype=int)
domain_labels[(blood_glucose >= 140.0) & (blood_glucose <= 200.0)] = 1
domain_labels[blood_glucose > 200.0] = 2
domain_labels_tensor = torch.tensor(domain_labels, dtype=torch.long).to(device)

# 数据集划分
train_idx, val_idx = train_test_split(np.arange(len(features)), test_size=0.2, random_state=42, stratify=type_labels_resampled)
train_dataset = TensorDataset(features_tensor[train_idx], type_labels_tensor[train_idx], treatment_labels_tensor[train_idx], domain_labels_tensor[train_idx])
val_dataset = TensorDataset(features_tensor[val_idx], type_labels_tensor[val_idx], treatment_labels_tensor[val_idx], domain_labels_tensor[val_idx])
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

# 梯度反转层
class GradientReversal(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.alpha * grad_output, None

# 模型变体定义
class EnhancedDiabetesTypeModelBase(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_types, num_treatments, num_domains=3, use_attention=True, use_domain=True):
        super(EnhancedDiabetesTypeModelBase, self).__init__()
        self.shared_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim * 4),
            nn.BatchNorm1d(hidden_dim * 4),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim * 4, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        self.use_attention = use_attention
        if use_attention:
            self.attention = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.Tanh(),
                nn.Linear(hidden_dim // 2, hidden_dim),
                nn.Softmax(dim=1)
            )
        self.type_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.BatchNorm1d(hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 4, num_types)
        )
        self.treatment_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_treatments)
        )
        if use_domain:
            self.domain_classifier = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim, num_domains)
            )
        self._initialize_weights()
    
    def _initialize_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    
    def forward(self, x, alpha=1.0):
        shared_features = self.shared_layer(x)
        if self.use_attention:
            attention_weights = self.attention(shared_features)
            weighted_features = shared_features * attention_weights
        else:
            weighted_features = shared_features
        type_logits = self.type_head(weighted_features)
        treatment_logits = self.treatment_head(weighted_features)
        if hasattr(self, 'domain_classifier'):
            domain_logits = self.domain_classifier(GradientReversal.apply(weighted_features, alpha))
            return type_logits, treatment_logits, domain_logits
        return type_logits, treatment_logits, None

# 初始化模型变体
input_dim = len(selected_features)
hidden_dim = 512
num_types = len(type_to_idx)
num_treatments = len(treatment_to_idx)

# 变体 1: 原始模型 (有注意力机制和域对抗)
model_full = EnhancedDiabetesTypeModelBase(input_dim, hidden_dim, num_types, num_treatments, use_attention=True, use_domain=True).to(device)

# 变体 2: 无注意力机制
model_no_attention = EnhancedDiabetesTypeModelBase(input_dim, hidden_dim, num_types, num_treatments, use_attention=False, use_domain=True).to(device)

# 变体 3: 无域对抗
model_no_domain = EnhancedDiabetesTypeModelBase(input_dim, hidden_dim, num_types, num_treatments, use_attention=True, use_domain=False).to(device)

# 损失函数
type_weights = torch.zeros(num_types, dtype=torch.float32)
for idx in range(num_types):
    count = (type_labels_resampled == idx).sum()
    type_weights[idx] = len(type_labels_resampled) / (num_types * count) if count > 0 else 1.0
type_criterion = nn.CrossEntropyLoss(weight=type_weights.to(device))

treatment_weights = torch.zeros(num_treatments, dtype=torch.float32)
for idx in range(num_treatments):
    count = (treatment_labels_resampled == idx).sum()
    treatment_weights[idx] = len(treatment_labels_resampled) / (num_treatments * count) if count > 0 else 1.0
treatment_criterion = nn.CrossEntropyLoss(weight=treatment_weights.to(device))

domain_criterion = nn.CrossEntropyLoss()

# 训练函数
def train_model(model, train_loader, val_loader, model_name):
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    warmup_scheduler = WarmUpScheduler(optimizer, warmup_epochs=5, base_lr=0.001)
    scheduler = CosineAnnealingLR(optimizer, T_max=20)
    best_val_loss = float('inf')
    patience = 5
    counter = 0
    best_model_path = f"/personal/new_models/best_{model_name}_dataset8_diabetes_type_model.pth"
    final_model_path = f"/personal/new_models/{model_name}_dataset8_diabetes_type_model.pth"

    history = {'train_loss': [], 'val_loss': [], 'type_acc': [], 'treat_acc': [], 'type_f1': [], 'treat_f1': []}
    type_metrics_history = []

    for epoch in range(50):
        model.train()
        total_train_loss = 0.0
        train_type_preds = []
        train_type_targets = []
        train_treat_preds = []
        train_treat_targets = []
        
        for batch_features, batch_type_labels, batch_treat_labels, batch_domain_labels in train_loader:
            batch_features = batch_features.to(device)
            batch_type_labels = batch_type_labels.to(device)
            batch_treat_labels = batch_treat_labels.to(device)
            
            optimizer.zero_grad()
            
            alpha = 2.0 / (1.0 + np.exp(-10 * epoch / 50)) - 1 if hasattr(model, 'domain_classifier') else 0.0
            type_logits, treatment_logits, domain_logits = model(batch_features, alpha)
            
            loss_type = type_criterion(type_logits, batch_type_labels)
            loss_treatment = treatment_criterion(treatment_logits, batch_treat_labels)
            loss = loss_type + 0.5 * loss_treatment
            if domain_logits is not None:
                loss_domain = domain_criterion(domain_logits, batch_domain_labels)
                loss += 0.1 * loss_domain
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_train_loss += loss.item()
            
            train_type_preds.extend(torch.argmax(type_logits, dim=1).cpu().numpy())
            train_type_targets.extend(batch_type_labels.cpu().numpy())
            train_treat_preds.extend(torch.argmax(treatment_logits, dim=1).cpu().numpy())
            train_treat_targets.extend(batch_treat_labels.cpu().numpy())
        
        avg_train_loss = total_train_loss / len(train_loader)
        train_type_acc = accuracy_score(train_type_targets, train_type_preds)
        train_treat_acc = accuracy_score(train_treat_targets, train_treat_preds)
        
        model.eval()
        total_val_loss = 0.0
        val_type_preds = []
        val_type_targets = []
        val_treat_preds = []
        val_treat_targets = []
        
        with torch.no_grad():
            for batch_features, batch_type_labels, batch_treat_labels, batch_domain_labels in val_loader:
                batch_features = batch_features.to(device)
                batch_type_labels = batch_type_labels.to(device)
                batch_treat_labels = batch_treat_labels.to(device)
                
                alpha = 2.0 / (1.0 + np.exp(-10 * epoch / 50)) - 1 if hasattr(model, 'domain_classifier') else 0.0
                type_logits, treatment_logits, domain_logits = model(batch_features, alpha)
                
                loss_type = type_criterion(type_logits, batch_type_labels)
                loss_treatment = treatment_criterion(treatment_logits, batch_treat_labels)
                loss = loss_type + 0.5 * loss_treatment
                if domain_logits is not None:
                    loss_domain = domain_criterion(domain_logits, batch_domain_labels)
                    loss += 0.1 * loss_domain
                
                total_val_loss += loss.item()
                
                val_type_preds.extend(torch.argmax(type_logits, dim=1).cpu().numpy())
                val_type_targets.extend(batch_type_labels.cpu().numpy())
                val_treat_preds.extend(torch.argmax(treatment_logits, dim=1).cpu().numpy())
                val_treat_targets.extend(batch_treat_labels.cpu().numpy())
        
        avg_val_loss = total_val_loss / len(val_loader)
        val_type_acc = accuracy_score(val_type_targets, val_type_preds)
        val_treat_acc = accuracy_score(val_treat_targets, val_treat_preds)
        val_type_f1 = f1_score(val_type_targets, val_type_preds, average='weighted')
        val_treat_f1 = f1_score(val_treat_targets, val_treat_preds, average='weighted')
        
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['type_acc'].append(val_type_acc)
        history['treat_acc'].append(val_treat_acc)
        history['type_f1'].append(val_type_f1)
        history['treat_f1'].append(val_treat_f1)
        
        print(f"\n{model_name} - Epoch {epoch+1}/50")
        print(f"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
        print(f"Type Acc: {val_type_acc:.4f}, Treat Acc: {val_treat_acc:.4f}")
        print(f"Type F1: {val_type_f1:.4f}, Treat F1: {val_treat_f1:.4f}")
        
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            counter = 0
            torch.save(model.state_dict(), best_model_path)
            print(f"New best model (Val Loss: {best_val_loss:.4f})")
        else:
            counter += 1
            if counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break
        
        warmup_scheduler.step()
        if epoch >= 5:
            scheduler.step()
    
    shutil.copy(best_model_path, final_model_path)
    if os.path.exists(best_model_path):
        os.remove(best_model_path)
    print(f"Model saved to: {final_model_path}")
    
    return history

# 训练三个变体
models = [
    ('full_model', model_full),
    ('no_attention', model_no_attention),
    ('no_domain', model_no_domain)
]

all_histories = {}
for model_name, model in models:
    print(f"\nTraining {model_name}...")
    history = train_model(model, train_loader, val_loader, model_name)
    all_histories[model_name] = history

# 对比结果
print("\nAblation Study Results (Best Validation Metrics):")
for model_name, history in all_histories.items():
    best_idx = np.argmin(history['val_loss'])
    print(f"{model_name}:")
    print(f"  Val Loss: {history['val_loss'][best_idx]:.4f}")
    print(f"  Type Acc: {history['type_acc'][best_idx]:.4f}")
    print(f"  Treat Acc: {history['treat_acc'][best_idx]:.4f}")
    print(f"  Type F1: {history['type_f1'][best_idx]:.4f}")
    print(f"  Treat F1: {history['treat_f1'][best_idx]:.4f}")